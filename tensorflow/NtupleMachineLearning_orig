#!/usr/bin/env python

# Import needed python packages:
import sys
import os
import re
import imp
import glob
import time
import subprocess
import argparse
import tensorflow as tf
import numpy as np
import pylab as P
import matplotlib.pyplot as plt

# python 2.4 does not have any and all
try: any, all
except NameError:
    any = lambda x: reduce(lambda a,b: a or b, x)
    all = lambda x: reduce(lambda a,b: a and b, x)

###

class GlobalParameters():
    """
    Global class for data parameters. This includes:
    
    Methods:
    index_in_epoch -- returns XXX
    epochs_completed -- retuns YYY
    num_examples -- return the number of training examples

    """
    
    def __init__(self):
        self._index_in_epoch = 0
        self._epochs_completed = 0
        self._num_examples = 0

    def index_in_epoch(self):
        return self._index_in_epoch

    def epochs_completed(self):
        return self._epochs_completed

    def num_examples(self):
        return self._num_examples


    
def root_init(parameters):
    """
    Initializes ROOT stuff and compiles ROOT libraries

    Arguments:
    parameters -- InputParameter type variable with command line input parameters

    Returns:
    -

    """
    global ROOT
    try:
        ROOT
        return
    except NameError:
        pass
    import ROOT

    # add NtupleAnalyzer directory to macro path
    try:
        ntupleanalyzer_path = os.path.abspath(os.path.dirname(__file__))
        if not ntupleanalyzer_path:
            ntupleanalyzer_path = parameters.sourcepath
            if not ntupleanalyzer_path:
                raise ValueError('Empty path to NtupleAnalyzer source: add it to input with --sourcepath=<your_path>')
    except ValueError as e:
        print (e)
        sys.exit(2)

    ROOT.gROOT.SetMacroPath(ROOT.gROOT.GetMacroPath().rstrip(':') + ':' + ntupleanalyzer_path)
    ROOT.gSystem.AddIncludePath("-Wno-deprecated-declarations")

    ROOT.gSystem.Load("libRIO.so")
    ROOT.gSystem.Load("libTreePlayer.so")
    ROOT.gPluginMgr.AddHandler("TVirtualStreamerInfo", "*", "TStreamerInfo", "RIO", "TStreamerInfo()")
    ROOT.gPluginMgr.AddHandler("TVirtualTreePlayer", "*", "TTreePlayer", "TreePlayer", "TTreePlayer()");

    ROOT.gSystem.Load("libfastjet.so")
    ROOT.gROOT.LoadMacro("TSelectorMain.C+")
    ROOT.gROOT.LoadMacro("TSelectorAnalyzer.C+")
    ROOT.gROOT.LoadMacro("TSelectorReader.C+")


def makeplot(title, x, weights, ggf_size, y):
    tot_size=len(y)
    #P.figure()

    #f, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, sharex='col', sharey='row')
    #ax1.set_title(title+"_ggf")
    #ax1.set_yscale("log")
    #ax1.set_xlim(0.0,1000.0)
    #ax1.set_ylim(1E0,1E5)
    #x_ggf=x[0:ggf_size]
    #my_bins=[0.,10.,20.,30.,40.,50.,60.,70.,80.,90.,100.,110.,120.,130.,140.,150.,160.,170.,180.,190.,200.,210.,220.,230.,
             #240.,250.,260.,270.,280.,290.,300.,310.,320.,330.,340.,350.,360.,370.,380.,390.,400.,410.,420.,430.,440.,
             #450.,460.,470.,480.,490.,500.,510.,520.,530.,540.,550.,560.,570.,580.,590.,600.,610.,620.,630.,640.,650.,
             #660.,670.,680.,690.,700,710.,720.,730.,740.,750.,760.,770.,780.,790.,800.,810.,820.,830.,840.,850.,860.,
             #870.,880.,890.,900.,910.,920.,930.,940.,950.,960.,970.,980.,990.]
    my_bins_pt=[0.,20.,40.,60.,80.,100.,120.,140.,160.,180.,200.,220.,240.,260.,280.,300.,320.,340.,360.,380.,400.,420.,440.,
             460.,480.,500.,520.,540.,560.,580.,600.,620.,640.,660.,680.,700,720.,740.,760.,780.,800.,820.,840.,860.,
             880.,900.,920.,940.,960.,980.]
    my_bins_m=[0.,40.,80.,120.,160.,200.,240.,280.,320.,360.,400.,440.,480.,520.,560.,600.,640.,680.,720.,760.,800.,840.,880.,920.,960.,1000.,1040.,1080.,1120.,1160.,1200.,1240.,1280.,1320.,1360.,1400.,1440.,1480.,1520.,1560.,1600.,1640.,1680.,1720.,1760.,1800.,1840.,1880.,1920.,1960.]
    my_bins_y=[-4.5,-4.05,-3.6,-3.15,-2.7,-2.25,-1.8,-1.35,-0.9,-0.45,0.,0.45,0.9,1.35,1.8,2.25,2.7,3.15,3.6,4.05]
    my_bins_dphi=[0.0,0.157,0.314,0.471,0.627,0.785,0.942,1.1,1.257,1.414,1.571,1.728,1.885,2.042,2.199,2.356,2.513,2.67,2.827,2.984]
    if title.lower().find("pt")>=0:
        my_bins=my_bins_pt
        lowlimit=0.0
        uplimit=1000.0
    if title.lower().find("mj")>=0:
       my_bins=my_bins_m
       lowlimit=0.0
       uplimit=2000.0
    if title.lower().find("yj")>=0:
       my_bins=my_bins_y
       lowlimit=-4.5
       uplimit=4.5
    if title.lower().find("dphi")>=0:
        my_bins=my_bins_dphi
        lowlimit=0.0
        uplimit=3.1415
    #weights_ggf=[w / float(ggf_size) for w in weights[0:ggf_size]]
    #weights_ggf=np.ones(ggf_size)
    #n_ggf, bins, patches = ax1.hist( x_ggf, bins=my_bins, weights=weights_ggf,histtype='bar', alpha=0.5)
    #print n_ggf
    #print bins
    #print x_ggf
    #print weights[0:ggf_size]
    #plt.show()
    f, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, sharex='col', sharey='row')
    ax1.set_title(title+"_ggf")
    ax1.set_yscale("log")
    ax1.set_xlim(lowlimit,uplimit)
    ax1.set_ylim(1E0,1E5)
    x_ggf=x[0:ggf_size]

    weights_ggf=np.ones(ggf_size)
    n_ggf, bins, patches = ax1.hist( x_ggf, bins=my_bins, weights=weights_ggf, histtype='bar', alpha=0.5)

    ax2.set_title(title+"_vbf")
    ax2.set_yscale("log")
    ax2.set_xlim(lowlimit,uplimit)
    ax2.set_ylim(1E0,1E5)
    x_vbf=x[ggf_size+1:tot_size]
    vbf_size=tot_size-ggf_size
    #weights_vbf=[w / float(vbf_size) for w in weights[ggf_size+1:tot_size]]
    #weights_vbf=np.ones(vbf_size+1)
    weights_vbf=[w/w for w in weights[ggf_size+1:tot_size]]
    n_vbf, bins, patches = ax2.hist( x_vbf, bins=my_bins, weights=weights_vbf, histtype='bar', alpha=0.5)
    #plt.show()

    xggf_rec=[]
    weights_ggf_rec=[]
    xvbf_rec=[]
    weights_vbf_rec=[]
    ggf_size_rec=0
    vbf_size_rec=0
    for i in range(len(y)):
        if y[i][0]==1.0:
            xggf_rec.append(x[i])
            weights_ggf_rec.append(weights[i])
            ggf_size_rec+=1
        else:
            xvbf_rec.append(x[i])
            weights_vbf_rec.append(weights[i])
            vbf_size_rec+=1

    ax3.set_title(title+"_ggf_rec")
    ax3.set_yscale("log")
    ax3.set_xlim(lowlimit,uplimit)
    ax3.set_ylim(1E0,1E5)
    #weights_ggf_rec_norm=[w/float(ggf_size_rec) for w in weights_ggf_rec]
    weights_ggf_rec_norm=np.ones(ggf_size_rec)
    n_ggf_rec, bins, patches = ax3.hist( xggf_rec, bins=my_bins, weights=weights_ggf_rec_norm, histtype='bar', alpha=0.5)
    #plt.show()

    ax4.set_title(title+"_vbf_rec")
    ax4.set_yscale("log")
    ax4.set_xlim(lowlimit,uplimit)
    ax4.set_ylim(1E0,1E5)
    #weights_vbf_rec_norm=[w/float(vbf_size_rec) for w in weights_vbf_rec]
    weights_vbf_rec_norm=np.ones(vbf_size_rec)
    n_vbf_rec, bins, patches = ax4.hist( xvbf_rec, bins=my_bins, weights=weights_vbf_rec_norm, histtype='bar', alpha=0.5)



    ratio_ggf=[]
    ratio_vbf=[]
    #print n_ggf
    #print n_ggf_rec

    for i in range(len(bins)-1):
        if n_ggf_rec[i]!=0.0:
            ratio_ggf.append(n_ggf[i]/n_ggf_rec[i])
        else:
            ratio_ggf.append(0.0)
        if n_vbf_rec[i]!=0.0:
            ratio_vbf.append(n_vbf[i]/n_vbf_rec[i])
        else:
            ratio_vbf.append(0.0)


    print "n_ggf"
    print n_ggf
    print "n_vbf"
    print n_vbf
    print "n_ggf_rec"
    print n_ggf_rec
    print "n_vbf_rec"
    print n_vbf_rec
    print "ratio_ggf"
    print ratio_ggf
    print "ratio_vbf"
    print ratio_vbf
    #print ratio_ggf
    #print ratio_vbf
    ax5.set_title("ratio_ggf")
    ax5.set_xlim(lowlimit,uplimit)
    ax5.set_ylim(0.0,2.0)
    #n_ggf_ratio, bins, patches = ax5.hist(ratio_ggf, bins=my_bins, histtype='step')
    #print my_bins[:-1]
    #print ratio_ggf
    ax5.plot(my_bins[:-1], ratio_ggf)

    ax6.set_title("ratio_vbf")
    #ax6.set_xlim(0.0,1000.0)
    ax6.set_ylim(0.0,2.0)
    #n_vbf_ratio, bins, patches = ax6.hist(ratio_vbf, bins=my_bins, histtype='step')
    ax6.plot(my_bins[:-1], ratio_vbf)

    plt.show()

def make2dplot(observables, data, ggf_size, tot_size):

    for j in range(0, len(observables)):
        plt.figure(j+1, figsize=(18,8))
        k=0
        for i in range(0, len(observables)):
            if i == j:
                continue
            k+=1
            plt.subplot(250+k)
            plt.plot(data[j, 0:ggf_size], data[i, 0:ggf_size], 'r.', data[j, ggf_size:tot_size], data[i, ggf_size:tot_size], 'b.', markersize=1)
            plt.xlabel(observables[j])
            plt.ylabel(observables[i])
            plt.title('Plot: '+observables[j]+' vs. '+observables[i])
        plt.tight_layout()
        plt.savefig('Figure'+str(j+1)+'.png')
    #plt.show()


def load_data(ip, gp, mode):
    """
    Loads data from ROOT NTuples both for training and for test sets

    Arguments:
    ip -- InputParameter type variable
    gp -- GlobalParameter type variable
    mode -- string defining the running more ( can be set equal to 'train' or 'test')

    Returns:
    x_vec -- numpy-array of shape (n_x, m) with input vectors: 
             m   : number of examples loaded from ntuples (which depends on the applied analysis cuts) 
             n_x : input dimension (for H+2j n_x = 11, H+3j n_x = 12)
    y_vec -- numpy-array of shape (2, m) with output vectors
             m   : number of examples loaded from ntuples (which depends on the applied analysis cuts)
             -> output is a 2-dim one-hot vector (1 0) =: ggf , (0 1) =: vbf
    """

    # Define reader selector:
    TReader = ROOT.TSelectorReader()

    # Analysis Selectors:
    AnalyzerSelector = ROOT.TSelectorAnalyzer()
    AnalyzerSelector.multip = ip.multip
    TReader.addSelector(AnalyzerSelector)

    proc_type=[]
    pthlist=[]
    ptj1list=[]
    ptj2list=[]
    mjjlist=[]
    dphijjlist=[]
    yj1list=[]
    yj2list=[]
    yjjlist=[]
    zstarlist=[]
    zstarj3list=[]
    Rptjetlist=[]
    weights=[]
    observable_list=[]

    #// Define chain and add file list:
    chain = ROOT.TChain("t3")

    if mode == 'train':
        chain.Add(ip.GGFFILE_TRAIN)
    elif mode == 'devel':
        chain.Add(ip.GGFFILE_EVAL)
    else:
        raise ValueError("Not valid mode. Mode should be 'train' or 'devel'.")

    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if ip.events < 0:
      chain.Process(TReader, "", chain.GetMaxEntryLoop(), 0)
    else:
      chain.Process(TReader, "", 10*int(ip.events), 0)

    ggf_size = len(AnalyzerSelector.pth)

    chain = ROOT.TChain("t3")
    if mode == 'train':    
        chain.Add(ip.VBFFILE_TRAIN)
    elif mode == 'devel':
        chain.Add(ip.VBFFILE_EVAL)
    else:
        raise ValueError("Not valid mode. Mode should be 'train' or 'devel'.")

    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if ip.events < 0:
      chain.Process(TReader, "", chain.GetMaxEntryLoop(), 0)
    else:
      chain.Process(TReader, "", int(ip.events), 0)

    tot_size = len(AnalyzerSelector.pth)
    vbf_size = tot_size-ggf_size
    
    gp._num_examples = tot_size

    for i in range(ggf_size):
        proc_type.append([1.,0.])
    for i in range(vbf_size):
        proc_type.append([0.,1.])

    for i in range(tot_size):
        pthlist.append(AnalyzerSelector.pth[i])
        ptj1list.append([AnalyzerSelector.ptj1[i]])
        ptj2list.append([AnalyzerSelector.ptj2[i]])
        mjjlist.append([AnalyzerSelector.mjj[i]])
        dphijjlist.append([AnalyzerSelector.dphijj[i]])
        yj1list.append([AnalyzerSelector.yj1[i]])
        yj2list.append([AnalyzerSelector.yj2[i]])
        yjjlist.append([AnalyzerSelector.yjj[i]])
        zstarlist.append([AnalyzerSelector.zstar[i]])
        Rptjetlist.append([AnalyzerSelector.Rptjet[i]])
        weights.append([AnalyzerSelector.me_weight[i]])

        if ip.multip == 2:
            observable_list.append([AnalyzerSelector.pth[i],AnalyzerSelector.ptj1[i],
                                          AnalyzerSelector.ptj2[i],AnalyzerSelector.mjj[i],AnalyzerSelector.dphijj[i],
                                          AnalyzerSelector.yj2[i],AnalyzerSelector.yj2[i],AnalyzerSelector.yjj[i],
                                          AnalyzerSelector.zstar[i],AnalyzerSelector.Rptjet[i],AnalyzerSelector.me_weight[i]])

        if ip.multip == 3:
            zstarj3list.append([AnalyzerSelector.zstarj3[i]])
            observable_list.append([AnalyzerSelector.pth[i],AnalyzerSelector.ptj1[i],
                                          AnalyzerSelector.ptj2[i],AnalyzerSelector.mjj[i],AnalyzerSelector.dphijj[i],
                                          AnalyzerSelector.yj2[i],AnalyzerSelector.yj2[i],AnalyzerSelector.yjj[i],
                                          AnalyzerSelector.zstar[i],AnalyzerSelector.Rptjet[i],AnalyzerSelector.zstarj3[i],
                                          AnalyzerSelector.me_weight[i]])

    # So far all ggf data are first and all vbf data are after
    # Reshuffle data such that ggf and vbf events appear alternating:
    perm = np.arange(gp.num_examples())
    np.random.seed(1)
    np.random.shuffle(perm)
    x_vec = np.array(observable_list)[perm]
    y_vec = np.array(proc_type)[perm]
    
    return x_vec.T, y_vec.T, ggf_size, vbf_size
    
def normalize_input(x_vec):

    n_obs = x_vec.shape[0]

    x_vec_norm = np.zeros(x_vec.shape)
    
    for i in range(n_obs):
        obsmax = np.amax(x_vec[i, :])
        x_vec_norm[i, :] = x_vec[i, :]/obsmax

    return x_vec_norm

def main(ip):

    # Store starting time:
    start_time = time.time()

    # Initialize can compile in ROOT:
    root_init(ip)

    # We want to handle Ctrl+C
    sh = ROOT.TSignalHandler(ROOT.kSigInterrupt, False)
    sh.Add()
    sh.Connect("Notified()", "TROOT", ROOT.gROOT, "SetInterrupt()")

    #if ip.mode == 'training':
    gp=GlobalParameters()
    
    print "--> Load training data ..."

    x_train, y_train, ggf_size, vbf_size = load_data(ip, gp, 'train')

    print(" Size of ggf training sample: %i events" % ggf_size)
    print(" Size of vbf training sample: %i events" % vbf_size)

    make2dplot(["pth","ptj1","ptj2","mjj","dphijj","yj1","yj2","yjj","zstar","Rptjet"], x_train, ggf_size, gp._num_examples)

    #x_train_norm = normalize_input(x_train)
    
    print "--> Load development data ..."

    x_devel, y_devel, ggf_size, vbf_size = load_data(ip, gp, 'devel')

    print(" Size of ggf develop sample: %i events" % ggf_size)
    print(" Size of vbf develop sample: %i events" % vbf_size)

    #x_devel_norm = normalize_input(x_devel)

    y_final=train(gp, ip, x_train.T, y_train.T, x_devel.T, y_devel.T)
    #y_final=train(gp, ip, x_train_norm.T, y_train.T, x_devel_norm.T, y_devel.T)
    
    y_rewrite=[]
    proc_ind = tf.InteractiveSession().run(tf.argmax(y_final, 1))
    for i in range(len(proc_ind)):
        if proc_ind[i]==0:
            y_rewrite.append([1.,0.])
        else:
            y_rewrite.append([0.,1.0])

    #makeplot("ptH", pthlist_test, weights_test, ggf_size, y_rewrite)
    #makeplot("mj1j2", mjjlist_test, weights_test, ggf_size, y_rewrite)
    #makeplot("ptj1", ptj1list_test, weights_test, ggf_size, y_rewrite)
    #makeplot("yj1", yj1list_test, weights_test, ggf_size, y_rewrite)
    #makeplot("dphijj", dphijjlist_test, weights_test, ggf_size, y_rewrite)

    print "Run time: %d seconds" % (time.time() - start_time)

    sys.exit()

##--[ Tensorflow functions:
def weight_variable(name, shape):
    """Create a weight variable with appropriate initialization."""
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)
    #return tf.get_variable(name, shape, initializer = tf.contrib.layers.xavier_initializer(seed = 1))

def bias_variable(name, shape):
    """Create a bias variable with appropriate initialization."""
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)
    #return tf.get_variable(name, shape, initializer = tf.zeros_initializer())

def variable_summaries(var):
    """Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""
    with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean', mean)
        with tf.name_scope('stddev'):
            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
        tf.summary.scalar('stddev', stddev)
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))
        tf.summary.histogram('histogram', var)

def nn_layer(input_tensor, output_dim, input_dim, layer_name, act=tf.nn.relu):
    """Reusable code for making a simple neural net layer.
    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.
    It also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    """
    # Adding a name scope ensures logical grouping of the layers in the graph.
    with tf.name_scope(layer_name):
        # This Variable will hold the state of the weights for the layer
        with tf.name_scope('weights'):
            weights = weight_variable("W_"+layer_name, [input_dim, output_dim])
            variable_summaries(weights)
        with tf.name_scope('biases'):
            biases = bias_variable("b_"+layer_name, [output_dim])
            variable_summaries(biases)
        with tf.name_scope('Wx_plus_b'):
            preactivate = tf.add(tf.matmul(input_tensor, weights), biases)
            tf.summary.histogram('pre_activations', preactivate)
        activations = act(preactivate, name='activation')
        tf.summary.histogram('activations', activations)
        return activations

def next_batch(gp, batch_size, xs, ys):
    start = gp.index_in_epoch()
    if start + batch_size > gp.num_examples():
        #Finished epoch
        gp._epochs_completed +=1
        # Get the rest examples in this epoch
        rest_num_examples = gp.num_examples() - start
        xs_rest_part = xs[start:gp.num_examples()]
        ys_rest_part = ys[start:gp.num_examples()]
        
        start = 0
        gp._index_in_epoch = batch_size - rest_num_examples
        end = gp.index_in_epoch()
        xs_new_part = xs[start:end]
        ys_new_part = ys[start:end]
        xs=np.concatenate((xs_rest_part, xs_new_part), axis=0)
        ys=np.concatenate((ys_rest_part, ys_new_part), axis=0)
        return xs, ys
    else:
        gp._index_in_epoch += batch_size
        end = gp.index_in_epoch()
        return xs[start:end], ys[start:end]
    
##--] Tensorflow functions
    
def train(gp, ip, x_train, y_train, x_test, y_test):
    """
    

    """

    #print x_train.shape
    #print y_train.shape

    # print x_test.shape
    # print y_test.shape

    # Input/output layer dimensions
    n_x = x_train.shape[1]
    n_y = y_train.shape[1]
    
    #--> set random seed: only for testing purposes - TO BE REMOVED
    tf.set_random_seed(1)
  
    sess = tf.InteractiveSession(config=tf.ConfigProto(inter_op_parallelism_threads=8))

    # Input placeholders
    with tf.name_scope('input'):
        x  = tf.placeholder(tf.float32, [None, n_x], name='x-input')
        y_ = tf.placeholder(tf.float32, [None, n_y], name='y-input')

    # First FC hidden layer with 200 units
    hidden1 = nn_layer(x, 200, n_x, 'layer1')
    
    # Second FC hidden layer with 50 units
    hidden2 = nn_layer(hidden1, 50, 200, 'layer3')

    # Dropout layer
    with tf.name_scope('dropout'):
        keep_prob = tf.placeholder(tf.float32)
        tf.summary.scalar('dropout_keep_probability', keep_prob)
        dropped = tf.nn.dropout(hidden2, keep_prob)

    # Do not apply softmax activation yet, see below.
    y = nn_layer(dropped, n_y, 50, 'layer3', act=tf.identity)

    with tf.name_scope('cross_entropy'):
        # The raw formulation of cross-entropy,
        #
        # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)), reduction_indices=[1]))
        #
        # can be numerically unstable.So here we use
        # tf.nn.softmax_cross_entropy_with_logits on the raw outputs
        # of the nn_layer above, and then average across the batch.
        # --> will have to be substituted with tf.nn.softmax_cross_entropy_with_logits_v2 (see tf online manual)
        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)
        with tf.name_scope('total'):
            cross_entropy = tf.reduce_mean(diff)
    tf.summary.scalar('cross_entropy', cross_entropy)

    with tf.name_scope('train'):
        if ip.optimizer == 'Adam':
            train_step = tf.train.AdamOptimizer(ip.learning_rate).minimize(cross_entropy)
        elif ip.optimizer == 'Adagrad':
            train_step = tf.train.AdagradOptimizer(ip.learning_rate).minimize(cross_entropy)
        elif ip.optimizer == 'GraDe':
            train_step = tf.train.GradientDescentOptimizer(ip.learning_rate).minimize(cross_entropy)

    with tf.name_scope('accuracy'):
        with tf.name_scope('correct_prediction'):
            correct_prediction = tf.equal(tf.argmax(y, axis=1), tf.argmax(y_, axis=1))
        with tf.name_scope('accuracy'):
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar('accuracy', accuracy)

    # Merge all the summaries and write them out
    merged = tf.summary.merge_all()
    train_writer = tf.summary.FileWriter(ip.log_dir + '/train/'+ip.run_dir, sess.graph)
    devel_writer = tf.summary.FileWriter(ip.log_dir + '/devel/'+ip.run_dir)
    tf.global_variables_initializer().run()

    def feed_dict(gp, train):
        """Make a TensorFlow feed_dict: maps data onto Tensor placeholders."""
        if train:
            # use mini-batches of 128 events:
            xs, ys = next_batch(gp, 128, x_train, y_train)
            # use the full batch of training events:
            #xs, ys = x_train , y_train
            # dropout value
            k = ip.dropout
        else:
            xs, ys = x_test, y_test
            k = 1.0
        return {x: xs, y_: ys, keep_prob: k}

    # Train the model, and also write summaries.
    # Every 10th step, measure test-set accuracy, and write test summaries
    # All other steps, run train_step on training data, & add training summaries

    for i in range(ip.max_steps):
        if i % 10 == 0:  # Record summaries and test-set accuracy
            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(gp, False))
            devel_writer.add_summary(summary, i)
            print('Accuracy at step %s: %s' % (i, acc))

        else:  # Record train set summaries, and train
            if i % 100 == 99:  # Record execution stats
                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
                run_metadata = tf.RunMetadata()
                summary, _ = sess.run([merged, train_step],
                                      feed_dict=feed_dict(gp, True),
                                      options=run_options,
                                      run_metadata=run_metadata)
                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
                train_writer.add_summary(summary, i)
                print('Adding run metadata for', i)
            else:  # Record a summary
                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(gp,True))
                train_writer.add_summary(summary, i)
        if i==ip.max_steps-1:
            summary, acc, y, y_ = sess.run([merged, accuracy, y, y_], feed_dict=feed_dict(gp, False))
            ysoft= tf.nn.softmax(y)
            
            print "Total number of events " , len(x_test)
            nr_ggf=0
            nr_vbf=0
            nr_ggf_rec=0
            nr_vbf_rec=0
            for i in range(x_test.shape[0]):
                if y_[i, 0]==0.0:
                    nr_vbf +=1
                elif y_[i, 0]==1.0:
                    nr_ggf+=1
                if y[i, 0] > y[i, 1]:
                    nr_ggf_rec+=1
                elif y[i, 0]<y[i, 1]:
                    nr_vbf_rec+=1

            print "Number of GGF events ", nr_ggf
            print "Number of VBF events ", nr_vbf
            print "Number of reconstructed GGF events ", nr_ggf_rec
            print "Number of reconstructed VBF events ", nr_vbf_rec
            print ""        

            prob_range = [0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99]
        
            for prob in prob_range:
                x_new=[]
                y_new=[]
                yuscore_new=[]
                for i in range(len(x_test)):
                    if sess.run(ysoft)[i][0] >prob or sess.run(ysoft)[i][1] >prob:
                        x_new.append(x_test[i])
                        y_new.append(y[i])
                        yuscore_new.append(y_[i])

                nr_ggf_new=0
                nr_vbf_new=0
                nr_ggf_rec_new=0
                nr_vbf_rec_new=0
                for i in range(len(x_new)):
                    if yuscore_new[i][0]==0.0:
                        nr_vbf_new +=1
                    elif yuscore_new[i][0]==1.0:
                        nr_ggf_new+=1
                    if y_new[i][0]>y_new[i][1]:
                        nr_ggf_rec_new+=1
                    elif y_new[i][0]<y_new[i][1]:
                        nr_vbf_rec_new+=1

                print ""
                print " Results after cut on probability:", prob        
                print "Total number of events ", len(x_new)
                print "Number of GGF events ", nr_ggf_new
                print "Number of VBF events ", nr_vbf_new
                print "Number of reconstructed GGF events ", nr_ggf_rec_new
                print "Number of reconstructed VBF events ", nr_vbf_rec_new        

            y_new=np.array(y_new)
            yuscore_new=np.array(yuscore_new)
            correct_prediction = tf.equal(tf.argmax(y_new, 1), tf.argmax(yuscore_new, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
            print "accuracy_new", sess.run(accuracy)

    train_writer.close()
    devel_writer.close()

    return sess.run(ysoft)



class InputParameters:

    def __init__(self):

        ## Argument parser

        parser = argparse.ArgumentParser(description='NLO NTuples machine learning tool.') #,formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #subparser = parser.add_subparsers(dest='MODE', help='Program running mode: training, evaluation')

        #parser_train = subparser.add_parser('training', help='training help (add this mode for more specific help)')
        parser.add_argument("-m", "--multip", dest="MULTIP", required=True, help="Multiplicity of process to reweight: 2, 3")
        parser.add_argument("-e", "--events", dest="EVENTS", default=-1, help="Number of events to be processed [all]")
        parser.add_argument("--debug",  dest="DEBUG", default=False, action='store_const', const=True, help="Generate debug output [False]")
        parser.add_argument("-ggf_train", "--ggf_train", dest="GGFFILE_TRAIN", required=True, help="Root Ntuple for training GGF")
        parser.add_argument("-vbf_train", "--vbf_train", dest="VBFFILE_TRAIN", required=True, help="Root Ntuple for training VBF")
        parser.add_argument("-ggf_eval", "--ggf_eval", dest="GGFFILE_EVAL", required=True, help="Root Ntuple for evaluating GGF")
        parser.add_argument("-vbf_eval", "--vbf_eval", dest="VBFFILE_EVAL", required=True, help="Root Ntuple for evaluating VBF")
        parser.add_argument('--log_dir', type=str, dest='log_dir', default='/.th/pcl335a/scratch', help='Summaries log directory')
        parser.add_argument('--run_dir', type=str, dest='run_dir', default='run1', help='Log directory for single run')
        parser.add_argument('--dropout', type=float, dest="dropout", default=0.9, help='Keep probability for training dropout.')
        parser.add_argument('--max_steps', type=int, dest="max_steps", default=2000, help='Number of steps to run trainer.')
        parser.add_argument('--learning_rate', type=float, dest="learning_rate", default=0.0001, help='Initial learning rate')
        parser.add_argument('--optimizer', type=str, dest="optimizer", default="Adam", help='Optimizer to use, choice among: Adam, GraDe, Adagrad')

        args            = parser.parse_args()
        #self.mode       = args.MODE
        #self.filenames  = args.INPUTFILES
        self.multip     = int(args.MULTIP)
        self.events     = args.EVENTS
        self.debug      = args.DEBUG
        self.GGFFILE_TRAIN    = args.GGFFILE_TRAIN
        self.VBFFILE_TRAIN    = args.VBFFILE_TRAIN
        self.GGFFILE_EVAL    = args.GGFFILE_EVAL
        self.VBFFILE_EVAL    = args.VBFFILE_EVAL
        self.log_dir         = args.log_dir
        self.run_dir         = args.run_dir
        self.dropout         = args.dropout
        self.max_steps       = args.max_steps
        self.learning_rate   = args.learning_rate
        self.optimizer       = args.optimizer
        
        #if self.outfolder:
            #if not os.path.isdir(self.outfolder):
                #print "Output folder does not exist, creating it.."
                #os.makedirs(self.outfolder)

        try:
            value = int(self.multip)
        except ValueError:
            print "Multiplicity must be an integer: 2 or 3"
            sys.exit(2)

        #self.filenames.sort()


    def print_parameters(self):

        print "------------------------------"
        print "--    SETUP PARAMETERS      --"
        print "------------------------------"
        print ""
        print (" MODE: Higgs + {0} jet(s)".format( str(self.multip)))
        print ""
        #print " INPUT FILES:"
        #for i in self.filenames:
            #print " -", i
        #print ""
        #print " OUTPUT:"
        #if not self.outfolder:
            #print "   folder: not specified, use same as input-file folder"
        #else:
            #print "   folder: ", self.outfolder
        #if not self.outfile:
            #print "   suffix: ", self.suffix
        #else:
            #print "   file name: ", self.outfile
        #print ""
        #if self.events < 0:
            #print " EVENTS: all"
        #else:
            #print " EVENTS: ", self.events
        #print ""


if __name__ == '__main__':

    input_parameters = InputParameters()
    input_parameters.print_parameters()

    main(input_parameters)


