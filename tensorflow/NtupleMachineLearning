#!/usr/bin/env python

import sys
import os
import re
import imp
import glob
import time
import subprocess
import argparse
import tensorflow as tf
import numpy as np

# python 2.4 does not have any and all
try: any, all
except NameError:
    any = lambda x: reduce(lambda a,b: a or b, x)
    all = lambda x: reduce(lambda a,b: a and b, x)

###

class Globalparams():
    def __init__(self):
        self._index_in_epoch = 0
        self._epochs_completed = 0
        self._num_examples = 0
        
    def index_in_epoch(self):
        return self._index_in_epoch
    
    def epochs_completed(self):
        return self._epochs_completed
    
    def num_examples(self):
        return self._num_examples

def global_init(param):

    global ROOT
    try:
        ROOT
        return
    except NameError:
        pass
    import ROOT

    # add NtupleAnalyzer directory to macro path
    try:
        ntupleanalyzer_path = os.path.abspath(os.path.dirname(__file__))
        if not ntupleanalyzer_path:
            ntupleanalyzer_path = param.sourcepath
            if not ntupleanalyzer_path:
                raise ValueError('Empty path to NtupleAnalyzer source: add it to input with --sourcepath=<your_path>')
    except ValueError as e:
        print (e)
        sys.exit(2)

    ROOT.gROOT.SetMacroPath(ROOT.gROOT.GetMacroPath().rstrip(':') + ':' + ntupleanalyzer_path)
    ROOT.gSystem.AddIncludePath("-Wno-deprecated-declarations")



    ROOT.gSystem.Load("libRIO.so")
    ROOT.gSystem.Load("libTreePlayer.so")
    ROOT.gPluginMgr.AddHandler("TVirtualStreamerInfo", "*", "TStreamerInfo", "RIO", "TStreamerInfo()")
    ROOT.gPluginMgr.AddHandler("TVirtualTreePlayer", "*", "TTreePlayer", "TreePlayer", "TTreePlayer()");

    ROOT.gSystem.Load("libfastjet.so")
    # ROOT.gSystem.Load("libLHAPDF.so")
    # ROOT.gROOT.LoadMacro("LHAGlue.h+")
    ROOT.gROOT.LoadMacro("TSelectorMain.C+")
    ROOT.gROOT.LoadMacro("TSelectorAnalyzer.C+")
    #ROOT.gROOT.LoadMacro("TSelectorWrite.C+")
    ROOT.gROOT.LoadMacro("TSelectorReader.C+")



def main(param):

    # Store starting time:
    start_time = time.time()

    # Initialize can compile in ROOT:
    global_init(param)

    # We want to handle Ctrl+C
    sh = ROOT.TSignalHandler(ROOT.kSigInterrupt, False)
    sh.Add()
    sh.Connect("Notified()", "TROOT", ROOT.gROOT, "SetInterrupt()")

    # *****************************************************
    #// Perform trainigs:
    # *****************************************************

    #if param.mode == 'training':
        
    print"Training"
    gp=Globalparams()    
    




    #// Define reader selector:
    tr_reader = ROOT.TSelectorReader()

    #// Analysis Selectors:
    AnalyzerSelector = ROOT.TSelectorAnalyzer()
    tr_reader.addSelector(AnalyzerSelector)


    proc_type_train=[]
    pthlist_train=[]
    ptj1list_train=[]
    ptj2list_train=[]
    mjjlist_train=[]
    dphijjlist_train=[]
    yj1list_train=[]
    yj2list_train=[]
    observable_list_train=[]
    train_data=[]
       
    #// Define chain and add file list:
    chain = ROOT.TChain("t3")
    chain.Add(param.GGFFILE_TRAIN)
    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if param.events < 0:
      chain.Process(tr_reader, "", chain.GetMaxEntryLoop(), 0) 
    else:
      chain.Process(tr_reader, "", int(param.events), 0)   
    ggf_size=len(AnalyzerSelector.pth)

        
    chain = ROOT.TChain("t3")
    chain.Add(param.VBFFILE_TRAIN)
    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if param.events < 0:
      chain.Process(tr_reader, "", chain.GetMaxEntryLoop(), 0) 
    else:
      chain.Process(tr_reader, "", int(param.events), 0)   
    tot_size=len(AnalyzerSelector.pth)
    gp._num_examples = tot_size


    vbf_size=tot_size-ggf_size
        
    for i in range(ggf_size):
        proc_type_train.append([1.,0.])
    for i in range(vbf_size):
        proc_type_train.append([0.,1.])
            
        
    for i in range(tot_size):
        pthlist_train.append([AnalyzerSelector.pth[i]])
        ptj1list_train.append([AnalyzerSelector.ptj1[i]])
        ptj2list_train.append([AnalyzerSelector.ptj2[i]])
        mjjlist_train.append([AnalyzerSelector.mjj[i]])
        dphijjlist_train.append([AnalyzerSelector.dphijj[i]])
        yj1list_train.append([AnalyzerSelector.yj1[i]])
        yj2list_train.append([AnalyzerSelector.yj2[i]])
        observable_list_train.append([AnalyzerSelector.pth[i],AnalyzerSelector.ptj1[i],
                                AnalyzerSelector.ptj2[i],AnalyzerSelector.mjj[i],AnalyzerSelector.dphijj[i],
                                AnalyzerSelector.yj2[i],AnalyzerSelector.yj2[i]])
        
        
        
    #Shuffling the training data
    perm = np.arange(gp.num_examples())
    np.random.shuffle(perm)
    observable_list_train=np.array(observable_list_train)[perm]
    proc_type_train=np.array(proc_type_train)[perm]
        
        
               
    # Start training
    
    #x=tf.placeholder(tf.float32, [None, 7])
    #W = tf.Variable(tf.zeros([7,2]))
    #b = tf.Variable(tf.zeros([2]))
    #y = tf.nn.softmax(tf.matmul(x,W) +b)
        
    #y_ = tf.placeholder(tf.float32, [None,2])
    #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))
    #tf.summary.scalar('cross_entropy', cross_entropy)
    #train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
    #sess= tf.InteractiveSession()

    #merged=tf.summary.merge_all()
    #train_writer = tf.summary.FileWriter('/data/greiner/NTupleAnalyzer/tensorflow', sess.graph)    

    #correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    #tf.summary.scalar('accuracy', accuracy)
    #tf.summary.histogram('histogram', accuracy)
        
    #tf.global_variables_initializer().run()

    #print "ggf_size", ggf_size
    #print "vbf_size", vbf_size
    #for i in range(5):
     #batch_x = observable_list[i*2000:(i+1)*2000]
     #batch_y = proc_type[i*2000:(i+1)*2000]
     #sess.run(train_step, feed_dict={x: batch_x, y_: batch_y})
     #summary, accuracy = sess.run([merged,accuracy], feed_dict={x: batch_x, y_: batch_y})
     #train_writer.add_summary(summary,i)
     
    #sess.run(train_step, feed_dict={x: observable_list, y_: proc_type})     
           
    
    #train_writer.add_summary(merged)
        
    #print "correct_prediction", correct_prediction
        

    print"Evaluation"
    
    #// Define reader selector:
    eval_reader = ROOT.TSelectorReader()

    #// Analysis Selectors:
    AnalyzerSelector = ROOT.TSelectorAnalyzer()
    eval_reader.addSelector(AnalyzerSelector)    
    
    proc_type_test=[]
    pthlist_test=[]
    ptj1list_test=[]
    ptj2list_test=[]
    mjjlist_test=[]
    dphijjlist_test=[]
    yj1list_test=[]
    yj2list_test=[]    
    observable_list_test=[]
       
    #// Define chain and add file list:
    chain = ROOT.TChain("t3")
    chain.Add(param.GGFFILE_EVAL)
    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if param.events < 0:
      chain.Process(eval_reader, "", chain.GetMaxEntryLoop(), 0) 
    else:
      chain.Process(eval_reader, "", int(param.events), 0)   
    ggf_size=len(AnalyzerSelector.pth)

        
    chain = ROOT.TChain("t3")
    chain.Add(param.VBFFILE_EVAL)
    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if param.events < 0:
      chain.Process(eval_reader, "", chain.GetMaxEntryLoop(), 0) 
    else:
      chain.Process(eval_reader, "", int(param.events), 0)   
    tot_size=len(AnalyzerSelector.pth)


    vbf_size=tot_size-ggf_size
        
    for i in range(ggf_size):
        proc_type_test.append([1.,0.])
    for i in range(vbf_size):
        proc_type_test.append([0.,1.])
            
        
    for i in range(tot_size):
        pthlist_test.append([AnalyzerSelector.pth[i]])  
        ptj1list_test.append([AnalyzerSelector.ptj1[i]])
        ptj2list_test.append([AnalyzerSelector.ptj2[i]])
        mjjlist_test.append([AnalyzerSelector.mjj[i]])
        dphijjlist_test.append([AnalyzerSelector.dphijj[i]])
        yj1list_test.append([AnalyzerSelector.yj1[i]])
        yj2list_test.append([AnalyzerSelector.yj2[i]])        
        observable_list_test.append([AnalyzerSelector.pth[i],AnalyzerSelector.ptj1[i],
                                AnalyzerSelector.ptj2[i],AnalyzerSelector.mjj[i],AnalyzerSelector.dphijj[i],
                                AnalyzerSelector.yj2[i],AnalyzerSelector.yj2[i]])
        

    #print "tot_size", tot_size
    #print ""
    #print "Accuracy on test data:"
    #print(sess.run(accuracy, feed_dict={x: observable_list, y_: proc_type}))
    #print ""

    train(gp,param,observable_list_train, proc_type_train, observable_list_test, proc_type_test)


    print "Run time: %d seconds" % (time.time() - start_time)

    sys.exit()

    
    

def train(gp,param, x_train, y_train, x_test, y_test):
  # Import data
  #mnist = input_data.read_data_sets(FLAGS.data_dir,
                                    #one_hot=True,
                                    #fake_data=FLAGS.fake_data)

  print "in train"

                                  

  sess = tf.InteractiveSession()
  ## Create a multilayer model.

  ## Input placeholders
  with tf.name_scope('input'):
    x = tf.placeholder(tf.float32, [None, 7], name='x-input')
    y_ = tf.placeholder(tf.float32, [None, 2], name='y-input')

  ##with tf.name_scope('input_reshape'):
    ###image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])
    ###tf.summary.image('input', image_shaped_input, 10)
    ##image_shaped_input = tf.reshape(x, 7)
    ##tf.summary.image('input', image_shaped_input, 2)    

  # We can't initialize these variables to 0 - the network will get stuck.
  def weight_variable(shape):
    """Create a weight variable with appropriate initialization."""
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

  def bias_variable(shape):
    """Create a bias variable with appropriate initialization."""
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

  def variable_summaries(var):
    """Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""
    with tf.name_scope('summaries'):
      mean = tf.reduce_mean(var)
      tf.summary.scalar('mean', mean)
      with tf.name_scope('stddev'):
        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
      tf.summary.scalar('stddev', stddev)
      tf.summary.scalar('max', tf.reduce_max(var))
      tf.summary.scalar('min', tf.reduce_min(var))
      tf.summary.histogram('histogram', var)

  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
    """Reusable code for making a simple neural net layer.
    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.
    It also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    """
    # Adding a name scope ensures logical grouping of the layers in the graph.
    with tf.name_scope(layer_name):
      # This Variable will hold the state of the weights for the layer
      with tf.name_scope('weights'):
        weights = weight_variable([input_dim, output_dim])
        variable_summaries(weights)
      with tf.name_scope('biases'):
        biases = bias_variable([output_dim])
        variable_summaries(biases)
      with tf.name_scope('Wx_plus_b'):
        preactivate = tf.matmul(input_tensor, weights) + biases
        tf.summary.histogram('pre_activations', preactivate)
      activations = act(preactivate, name='activation')
      tf.summary.histogram('activations', activations)
      return activations

  hidden1 = nn_layer(x, 7, 500, 'layer1')

  with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    tf.summary.scalar('dropout_keep_probability', keep_prob)
    dropped = tf.nn.dropout(hidden1, keep_prob)

  # Do not apply softmax activation yet, see below.
  y = nn_layer(dropped, 500, 2, 'layer2', act=tf.identity)

  with tf.name_scope('cross_entropy'):
    # The raw formulation of cross-entropy,
    #
    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),
    #                               reduction_indices=[1]))
    #
    # can be numerically unstable.
    #
    # So here we use tf.nn.softmax_cross_entropy_with_logits on the
    # raw outputs of the nn_layer above, and then average across
    # the batch.
    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)
    with tf.name_scope('total'):
      cross_entropy = tf.reduce_mean(diff)
  tf.summary.scalar('cross_entropy', cross_entropy)

  with tf.name_scope('train'):
    train_step = tf.train.AdamOptimizer(param.learning_rate).minimize(
        cross_entropy)

  with tf.name_scope('accuracy'):
    with tf.name_scope('correct_prediction'):
      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    with tf.name_scope('accuracy'):
      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  tf.summary.scalar('accuracy', accuracy)

  ## Merge all the summaries and write them out to
  ## /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)
  merged = tf.summary.merge_all()
  train_writer = tf.summary.FileWriter(param.log_dir + '/train', sess.graph)
  test_writer = tf.summary.FileWriter(param.log_dir + '/test')
  tf.global_variables_initializer().run()

  # Train the model, and also write summaries.
  # Every 10th step, measure test-set accuracy, and write test summaries
  # All other steps, run train_step on training data, & add training summaries
  
  def next_batch( gp,batch_size, xs, ys):
    start = gp.index_in_epoch()
    
    if start + batch_size > gp.num_examples():
        #Finished epoch
        gp._epochs_completed +=1 
        # Get the rest examples in this epoch
        rest_num_examples = gp.num_examples() - start
        xs_rest_part = xs[start:gp.num_examples()]
        ys_rest_part = ys[start:gp.num_examples()]  
        
        start = 0
        gp._index_in_epoch = batch_size - rest_num_examples
        end = gp.index_in_epoch()
        xs_new_part = xs[start:end]
        ys_new_part = ys[start:end]
        xs=np.concatenate((xs_rest_part, xs_new_part), axis=0)
        ys=np.concatenate((ys_rest_part, ys_new_part), axis=0)
        return xs, ys
    else:
        gp._index_in_epoch += batch_size
        end = gp.index_in_epoch()
    return xs[start:end], ys[start:end]

  def feed_dict(gp,train):
    """Make a TensorFlow feed_dict: maps data onto Tensor placeholders."""
    if train: 
      xs, ys = next_batch(gp, 100, x_train, y_train)
      k = param.dropout
    else:
      xs, ys = x_test, y_test
      k = 1.0
    return {x: xs, y_: ys, keep_prob: k}

  for i in range(param.max_steps):
    if i % 10 == 0:  # Record summaries and test-set accuracy
      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(gp,False))
      test_writer.add_summary(summary, i)
      print('Accuracy at step %s: %s' % (i, acc))
    else:  # Record train set summaries, and train
      if i % 100 == 99:  # Record execution stats
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        summary, _ = sess.run([merged, train_step],
                              feed_dict=feed_dict(gp,True),
                              options=run_options,
                              run_metadata=run_metadata)
        train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
        train_writer.add_summary(summary, i)
        print('Adding run metadata for', i)
      else:  # Record a summary
        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(gp,True))
        train_writer.add_summary(summary, i)
  train_writer.close()
  test_writer.close()



class Parameters:

    def __init__(self):
        
        ## Argument parser

        parser = argparse.ArgumentParser(description='NLO NTuples machine learning tool.') #,formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #subparser = parser.add_subparsers(dest='MODE', help='Program running mode: training, evaluation')
        
        #parser_train = subparser.add_parser('training', help='training help (add this mode for more specific help)')
        parser.add_argument("-m", "--multip", dest="MULTIP", required=True, help="Multiplicity of process to reweight: 2, 3")        
        parser.add_argument("-e", "--events", dest="EVENTS", default=-1, help="Number of events to be processed [all]")        
        parser.add_argument("--debug",  dest="DEBUG", default=False, action='store_const', const=True, help="Generate debug output [False]")
        parser.add_argument("-ggf_train", "--ggf_train", dest="GGFFILE_TRAIN", required=True, help="Root Ntuple for training GGF")
        parser.add_argument("-vbf_train", "--vbf_train", dest="VBFFILE_TRAIN", required=True, help="Root Ntuple for training VBF")                
        parser.add_argument("-ggf_eval", "--ggf_eval", dest="GGFFILE_EVAL", required=True, help="Root Ntuple for evaluating GGF")
        parser.add_argument("-vbf_eval", "--vbf_eval", dest="VBFFILE_EVAL", required=True, help="Root Ntuple for evaluating VBF")        
        parser.add_argument('--learning_rate', type=float, dest="learning_rate", default=0.001,help='Initial learning rate')
        parser.add_argument('--log_dir', type=str, dest='log_dir', default='/data/greiner/NTupleAnalyzer/tensorflow',help='Summaries log directory')    
        parser.add_argument('--dropout', type=float, dest="dropout", default=0.9,help='Keep probability for training dropout.') 
        parser.add_argument('--max_steps', type=int, dest="max_steps", default=1000,
                      help='Number of steps to run trainer.')
        
        #parser_ana = subparser.add_parser('analysis', help='analysis help (add this mode for more specific help)')
        #parser_ana.add_argument("-m", "--multip", dest="MULTIP", required=True, help="Multiplicity of process to reweight: 1, 2, 3")
        #parser_ana.add_argument("-s", "--suffix", dest="SUFFIX", default="_new", help="Suffix for output file name [_new]")
        #parser_ana.add_argument("-o", "--output", dest="OUTPUT", default="plots.root", help="name of output file [plots.root]")
        #parser_ana.add_argument("-f", "--folder", dest="FOLDER", default="", help="Output folder if different from input ones")
        #parser_ana.add_argument("-e", "--events", dest="EVENTS", default=-1, help="Number of events to be processed [all]")
        #parser_ana.add_argument("--debug",  dest="DEBUG", default=False, action='store_const', const=True, help="Generate debug output [False]")
        #parser_ana.add_argument("--sourcepath", dest="SOURCEPATH", default="", help="Path to the source of NtupleAnalyzer code")

        #parser_rwgt = subparser.add_parser('reweight', help='reweight help (add this mode for more specific help)')
        #parser_rwgt.add_argument("-m", "--multip", dest="MULTIP", required=True, help="Multiplicity of process to reweight: 1, 2, 3")
        #parser_rwgt.add_argument("-s", "--suffix", dest="SUFFIX", default="_new", help="Suffix for output file name [_new]")
        #parser_rwgt.add_argument("-o", "--output", dest="OUTPUT", default="", help="Output file name. By default the input file name is used with suffix '_new'")
        #parser_rwgt.add_argument("-f", "--folder", dest="FOLDER", default="", help="Output folder if different from input ones")
        #parser_rwgt.add_argument("-e", "--events", dest="EVENTS", default=-1, help="Number of events to be processed [all]")
        #parser_rwgt.add_argument("--debug",  dest="DEBUG", default=False, action='store_const', const=True, help="Generate debug output [False]")
        #parser_rwgt.add_argument("--sourcepath", dest="SOURCEPATH", default="", help="Path to the source of NtupleAnalyzer code")
        #parser_rwgt.add_argument("--noolp",  dest="NOOLP", default=False, action='store_const', const=True, help="Run without link to OLP library [False]")

        #parser.add_argument('INPUTFILES', nargs='+', metavar='Ntuple.root', help='One or more Root NTuple input files. When more than one input file is given, the files are processed one after the other.')
        #parser.add_argument("-s","--show", action='store_true', dest="SHOW", default=False, help="show results in default web browser [NO]")
        #parser.add_argument("-w","--overwrite", action='store_true', dest="OVERWRITE", default=False, help="overwrite existing plots [NO]")
        #parser.add_argument("-n", "-j", "--num-threads",action="store", dest='NUM_THREADS', type=int, default=numcores, help="max number of threads to be used [%s]" % numcores)
        #parser.add_argument("-a", "--alpha", action="store", dest='ALPHA_BAND', type=float, default=0.2, help="transparency of error bands [0.2]")
        #parser.add_argument("-v", "--verbose", action="store_const", const=logging.DEBUG, dest="LOGLEVEL", default=logging.INFO, help="print debug (very verbose) messages [NO]")
        #parser.add_argument("-q", "--quiet", action="store_const", const=logging.WARNING, dest="LOGLEVEL", default=logging.INFO, help="be very quiet [NO]")
        #parser.add_argument("-p", "--prefix", dest="PREFIX", default="xxx", help="prefix to histogram file names")
    
        args            = parser.parse_args()
        #self.mode       = args.MODE
        #self.filenames  = args.INPUTFILES
        self.multip     = int(args.MULTIP)
        #self.suffix     = args.SUFFIX
        #self.outfile    = args.OUTPUT
        #self.outfolder  = args.FOLDER
        self.events     = args.EVENTS
        self.debug      = args.DEBUG
        #self.sourcepath = args.SOURCEPATH
        self.GGFFILE_TRAIN    = args.GGFFILE_TRAIN
        self.VBFFILE_TRAIN    = args.VBFFILE_TRAIN
        self.GGFFILE_EVAL    = args.GGFFILE_EVAL
        self.VBFFILE_EVAL    = args.VBFFILE_EVAL
        self.learning_rate   = args.learning_rate
        self.log_dir          = args.log_dir
        self.dropout         = args.dropout
        self.max_steps  = args.max_steps

        #if self.outfolder:
            #if not os.path.isdir(self.outfolder):
                #print "Output folder does not exist, creating it.."
                #os.makedirs(self.outfolder)

        try:
            value = int(self.multip)
        except ValueError:
            print "Multiplicity must be an integer: 2 or 3"
            sys.exit(2)
  
        #self.filenames.sort()


    def print_parameters(self):

        print "------------------------------"
        print "--    SETUP PARAMETERS      --"
        print "------------------------------"
        print ""
        print (" MODE: Higgs + {0} jet(s)".format( str(self.multip)))
        print ""
        #print " INPUT FILES:"
        #for i in self.filenames:
            #print " -", i
        #print ""
        #print " OUTPUT:"
        #if not self.outfolder:
            #print "   folder: not specified, use same as input-file folder"
        #else:
            #print "   folder: ", self.outfolder
        #if not self.outfile:
            #print "   suffix: ", self.suffix
        #else:
            #print "   file name: ", self.outfile
        #print ""
        #if self.events < 0:
            #print " EVENTS: all"
        #else:
            #print " EVENTS: ", self.events
        #print ""


if __name__ == '__main__':

    input_param = Parameters()
    input_param.print_parameters()

    main(input_param)


# Example of lauch command for reweighting:
#
#     ./NtupleAnalyzer reweight -m 2 --debug --events=10 --output=test.root EDNTuplesFiles/H2.0j_amegic_GGFHT_B_6500_pt25.0_eta4.5_CT10nlo_r100_100.root
#
#
