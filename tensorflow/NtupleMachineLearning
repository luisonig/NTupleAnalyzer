#!/usr/bin/env python

# Import needed python packages:
import sys
import os
import re
import imp
import glob
import time
import subprocess
import argparse
import tensorflow as tf
import numpy as np
import pylab as P
import matplotlib.pyplot as plt

# python 2.4 does not have any and all
try: any, all
except NameError:
    any = lambda x: reduce(lambda a,b: a or b, x)
    all = lambda x: reduce(lambda a,b: a and b, x)

###

class GlobalParameters():
    """
    Global class for data parameters. This includes:
    
    Methods:
    index_in_epoch -- returns XXX
    epochs_completed -- retuns YYY
    num_examples -- return the number of training examples

    """
    
    def __init__(self):
        self._index_in_epoch = 0
        self._epochs_completed = 0
        self._num_examples = 0

    def index_in_epoch(self):
        return self._index_in_epoch

    def epochs_completed(self):
        return self._epochs_completed

    def num_examples(self):
        return self._num_examples


    
def root_init(parameters):
    """
    Initializes ROOT stuff and compiles ROOT libraries

    Arguments:
    parameters -- InputParameter type variable with command line input parameters

    Returns:
    -

    """
    global ROOT
    try:
        ROOT
        return
    except NameError:
        pass
    import ROOT

    # add NtupleAnalyzer directory to macro path
    try:
        ntupleanalyzer_path = os.path.abspath(os.path.dirname(__file__))
        if not ntupleanalyzer_path:
            ntupleanalyzer_path = parameters.sourcepath
            if not ntupleanalyzer_path:
                raise ValueError('Empty path to NtupleAnalyzer source: add it to input with --sourcepath=<your_path>')
    except ValueError as e:
        print (e)
        sys.exit(2)

    ROOT.gROOT.SetMacroPath(ROOT.gROOT.GetMacroPath().rstrip(':') + ':' + ntupleanalyzer_path)
    ROOT.gSystem.AddIncludePath("-Wno-deprecated-declarations")



    ROOT.gSystem.Load("libRIO.so")
    ROOT.gSystem.Load("libTreePlayer.so")
    ROOT.gPluginMgr.AddHandler("TVirtualStreamerInfo", "*", "TStreamerInfo", "RIO", "TStreamerInfo()")
    ROOT.gPluginMgr.AddHandler("TVirtualTreePlayer", "*", "TTreePlayer", "TreePlayer", "TTreePlayer()");

    ROOT.gSystem.Load("libfastjet.so")
    ROOT.gROOT.LoadMacro("TSelectorMain.C+")
    ROOT.gROOT.LoadMacro("TSelectorAnalyzer.C+")
    ROOT.gROOT.LoadMacro("TSelectorReader.C+")


def makeplot(title, x, weights, ggf_size, y):
    tot_size=len(y)
    #P.figure()

    #f, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, sharex='col', sharey='row')
    #ax1.set_title(title+"_ggf")
    #ax1.set_yscale("log")
    #ax1.set_xlim(0.0,1000.0)
    #ax1.set_ylim(1E0,1E5)
    #x_ggf=x[0:ggf_size]
    #my_bins=[0.,10.,20.,30.,40.,50.,60.,70.,80.,90.,100.,110.,120.,130.,140.,150.,160.,170.,180.,190.,200.,210.,220.,230.,
             #240.,250.,260.,270.,280.,290.,300.,310.,320.,330.,340.,350.,360.,370.,380.,390.,400.,410.,420.,430.,440.,
             #450.,460.,470.,480.,490.,500.,510.,520.,530.,540.,550.,560.,570.,580.,590.,600.,610.,620.,630.,640.,650.,
             #660.,670.,680.,690.,700,710.,720.,730.,740.,750.,760.,770.,780.,790.,800.,810.,820.,830.,840.,850.,860.,
             #870.,880.,890.,900.,910.,920.,930.,940.,950.,960.,970.,980.,990.]
    my_bins_pt=[0.,20.,40.,60.,80.,100.,120.,140.,160.,180.,200.,220.,240.,260.,280.,300.,320.,340.,360.,380.,400.,420.,440.,
             460.,480.,500.,520.,540.,560.,580.,600.,620.,640.,660.,680.,700,720.,740.,760.,780.,800.,820.,840.,860.,
             880.,900.,920.,940.,960.,980.]
    my_bins_m=[0.,40.,80.,120.,160.,200.,240.,280.,320.,360.,400.,440.,480.,520.,560.,600.,640.,680.,720.,760.,800.,840.,880.,920.,960.,1000.,1040.,1080.,1120.,1160.,1200.,1240.,1280.,1320.,1360.,1400.,1440.,1480.,1520.,1560.,1600.,1640.,1680.,1720.,1760.,1800.,1840.,1880.,1920.,1960.]
    my_bins_y=[-4.5,-4.05,-3.6,-3.15,-2.7,-2.25,-1.8,-1.35,-0.9,-0.45,0.,0.45,0.9,1.35,1.8,2.25,2.7,3.15,3.6,4.05]
    my_bins_dphi=[0.0,0.157,0.314,0.471,0.627,0.785,0.942,1.1,1.257,1.414,1.571,1.728,1.885,2.042,2.199,2.356,2.513,2.67,2.827,2.984]
    if title.lower().find("pt")>=0:
        my_bins=my_bins_pt
        lowlimit=0.0
        uplimit=1000.0
    if title.lower().find("mj")>=0:
       my_bins=my_bins_m
       lowlimit=0.0
       uplimit=2000.0
    if title.lower().find("yj")>=0:
       my_bins=my_bins_y
       lowlimit=-4.5
       uplimit=4.5
    if title.lower().find("dphi")>=0:
        my_bins=my_bins_dphi
        lowlimit=0.0
        uplimit=3.1415
    #weights_ggf=[w / float(ggf_size) for w in weights[0:ggf_size]]
    #weights_ggf=np.ones(ggf_size)
    #n_ggf, bins, patches = ax1.hist( x_ggf, bins=my_bins, weights=weights_ggf,histtype='bar', alpha=0.5)
    #print n_ggf
    #print bins
    #print x_ggf
    #print weights[0:ggf_size]
    #plt.show()
    f, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, sharex='col', sharey='row')
    ax1.set_title(title+"_ggf")
    ax1.set_yscale("log")
    ax1.set_xlim(lowlimit,uplimit)
    ax1.set_ylim(1E0,1E5)
    x_ggf=x[0:ggf_size]

    weights_ggf=np.ones(ggf_size)
    n_ggf, bins, patches = ax1.hist( x_ggf, bins=my_bins, weights=weights_ggf, histtype='bar', alpha=0.5)

    ax2.set_title(title+"_vbf")
    ax2.set_yscale("log")
    ax2.set_xlim(lowlimit,uplimit)
    ax2.set_ylim(1E0,1E5)
    x_vbf=x[ggf_size+1:tot_size]
    vbf_size=tot_size-ggf_size
    #weights_vbf=[w / float(vbf_size) for w in weights[ggf_size+1:tot_size]]
    #weights_vbf=np.ones(vbf_size+1)
    weights_vbf=[w/w for w in weights[ggf_size+1:tot_size]]
    n_vbf, bins, patches = ax2.hist( x_vbf, bins=my_bins, weights=weights_vbf, histtype='bar', alpha=0.5)
    #plt.show()

    xggf_rec=[]
    weights_ggf_rec=[]
    xvbf_rec=[]
    weights_vbf_rec=[]
    ggf_size_rec=0
    vbf_size_rec=0
    for i in range(len(y)):
        if y[i][0]==1.0:
            xggf_rec.append(x[i])
            weights_ggf_rec.append(weights[i])
            ggf_size_rec+=1
        else:
            xvbf_rec.append(x[i])
            weights_vbf_rec.append(weights[i])
            vbf_size_rec+=1

    ax3.set_title(title+"_ggf_rec")
    ax3.set_yscale("log")
    ax3.set_xlim(lowlimit,uplimit)
    ax3.set_ylim(1E0,1E5)
    #weights_ggf_rec_norm=[w/float(ggf_size_rec) for w in weights_ggf_rec]
    weights_ggf_rec_norm=np.ones(ggf_size_rec)
    n_ggf_rec, bins, patches = ax3.hist( xggf_rec, bins=my_bins, weights=weights_ggf_rec_norm, histtype='bar', alpha=0.5)
    #plt.show()

    ax4.set_title(title+"_vbf_rec")
    ax4.set_yscale("log")
    ax4.set_xlim(lowlimit,uplimit)
    ax4.set_ylim(1E0,1E5)
    #weights_vbf_rec_norm=[w/float(vbf_size_rec) for w in weights_vbf_rec]
    weights_vbf_rec_norm=np.ones(vbf_size_rec)
    n_vbf_rec, bins, patches = ax4.hist( xvbf_rec, bins=my_bins, weights=weights_vbf_rec_norm, histtype='bar', alpha=0.5)



    ratio_ggf=[]
    ratio_vbf=[]
    #print n_ggf
    #print n_ggf_rec

    for i in range(len(bins)-1):
        if n_ggf_rec[i]!=0.0:
            ratio_ggf.append(n_ggf[i]/n_ggf_rec[i])
        else:
            ratio_ggf.append(0.0)
        if n_vbf_rec[i]!=0.0:
            ratio_vbf.append(n_vbf[i]/n_vbf_rec[i])
        else:
            ratio_vbf.append(0.0)


    print "n_ggf"
    print n_ggf
    print "n_vbf"
    print n_vbf
    print "n_ggf_rec"
    print n_ggf_rec
    print "n_vbf_rec"
    print n_vbf_rec
    print "ratio_ggf"
    print ratio_ggf
    print "ratio_vbf"
    print ratio_vbf
    #print ratio_ggf
    #print ratio_vbf
    ax5.set_title("ratio_ggf")
    ax5.set_xlim(lowlimit,uplimit)
    ax5.set_ylim(0.0,2.0)
    #n_ggf_ratio, bins, patches = ax5.hist(ratio_ggf, bins=my_bins, histtype='step')
    #print my_bins[:-1]
    #print ratio_ggf
    ax5.plot(my_bins[:-1], ratio_ggf)

    ax6.set_title("ratio_vbf")
    #ax6.set_xlim(0.0,1000.0)
    ax6.set_ylim(0.0,2.0)
    #n_vbf_ratio, bins, patches = ax6.hist(ratio_vbf, bins=my_bins, histtype='step')
    ax6.plot(my_bins[:-1], ratio_vbf)

    plt.show()

def make2dplot(observables,train,ggf_size,tot_size):


    data=np.array(train)

    for j in range(0,len(observables)):
        plt.figure(j+1,figsize=(18,8))
        k=0
        for i in range(0,len(observables)):
            if i == j:
                continue
            k+=1
            plt.subplot(250+k)
            plt.plot(data[0:ggf_size,j], data[0:ggf_size,i], 'r.', data[ggf_size:tot_size,j], data[ggf_size:tot_size,i], 'b.', markersize=1)
            plt.xlabel(observables[j])
            plt.ylabel(observables[i])
            plt.title('Plot: '+observables[j]+' vs. '+observables[i])
        plt.tight_layout()
        plt.savefig('Figure'+str(j+1)+'.png')
    #plt.show()


def load_data(ip, gp, mode):
    """
    Loads data from ROOT NTuples both for training and for test sets

    Arguments:
    ip -- InputParameter type variable
    gp -- GlobalParameter type variable
    mode -- string defining the running more ( can be set equal to 'train' or 'test')

    Returns:


    """

    
    #// Define reader selector:
    TReader = ROOT.TSelectorReader()

    #// Analysis Selectors:
    AnalyzerSelector = ROOT.TSelectorAnalyzer()
    AnalyzerSelector.multip = ip.multip
    TReader.addSelector(AnalyzerSelector)

    proc_type=[]
    pthlist=[]
    ptj1list=[]
    ptj2list=[]
    mjjlist=[]
    dphijjlist=[]
    yj1list=[]
    yj2list=[]
    yjjlist=[]
    zstarlist=[]
    zstarj3list=[]
    Rptjetlist=[]
    weights=[]
    observable_list=[]

    #// Define chain and add file list:
    chain = ROOT.TChain("t3")

    if mode == 'train':
        chain.Add(ip.GGFFILE_TRAIN)
    elif mode == 'devel':
        chain.Add(ip.GGFFILE_EVAL)
    else:
        raise ValueError("Not valid mode. Mode should be 'train' or 'devel'.")

    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if ip.events < 0:
      chain.Process(TReader, "", chain.GetMaxEntryLoop(), 0)
    else:
      chain.Process(TReader, "", 10*int(ip.events), 0)
    ggf_size=len(AnalyzerSelector.pth)


    chain = ROOT.TChain("t3")
    chain.Add(ip.VBFFILE_TRAIN)
    chain.GetFile()  # force opening of the first file
    chain.SetMaxEntryLoop(2**60)
    if ip.events < 0:
      chain.Process(TReader, "", chain.GetMaxEntryLoop(), 0)
    else:
      chain.Process(TReader, "", int(ip.events), 0)
    tot_size=len(AnalyzerSelector.pth)
    gp._num_examples = tot_size


    vbf_size=tot_size-ggf_size
    print "ggf_size train", ggf_size
    print "vbf_size train", vbf_size

    for i in range(ggf_size):
        proc_type.append([1.,0.])
    for i in range(vbf_size):
        proc_type.append([0.,1.])


    for i in range(tot_size):
        pthlist.append(AnalyzerSelector.pth[i])
        ptj1list.append([AnalyzerSelector.ptj1[i]])
        ptj2list.append([AnalyzerSelector.ptj2[i]])
        mjjlist.append([AnalyzerSelector.mjj[i]])
        dphijjlist.append([AnalyzerSelector.dphijj[i]])
        yj1list.append([AnalyzerSelector.yj1[i]])
        yj2list.append([AnalyzerSelector.yj2[i]])
        yjjlist.append([AnalyzerSelector.yjj[i]])
        zstarlist.append([AnalyzerSelector.zstar[i]])
        Rptjetlist.append([AnalyzerSelector.Rptjet[i]])
        weights.append([AnalyzerSelector.me_weight[i]])

        if ip.multip == 3:
            zstarj3list.append([AnalyzerSelector.zstarj3[i]])


        if ip.multip == 2:
            observable_list.append([AnalyzerSelector.pth[i],AnalyzerSelector.ptj1[i],
                                          AnalyzerSelector.ptj2[i],AnalyzerSelector.mjj[i],AnalyzerSelector.dphijj[i],
                                          AnalyzerSelector.yj2[i],AnalyzerSelector.yj2[i],AnalyzerSelector.yjj[i],
                                          AnalyzerSelector.zstar[i],AnalyzerSelector.Rptjet[i],AnalyzerSelector.me_weight[i]])

        if ip.multip == 3:
            observable_list.append([AnalyzerSelector.pth[i],AnalyzerSelector.ptj1[i],
                                          AnalyzerSelector.ptj2[i],AnalyzerSelector.mjj[i],AnalyzerSelector.dphijj[i],
                                          AnalyzerSelector.yj2[i],AnalyzerSelector.yj2[i],AnalyzerSelector.yjj[i],
                                          AnalyzerSelector.zstar[i],AnalyzerSelector.Rptjet[i],AnalyzerSelector.zstarj3[i],
                                          AnalyzerSelector.me_weight[i]])

    # So far all ggf data are first and all vbf data are after
    # Reshuffle data such that ggf and vbf events appear alternating:
    perm = np.arange(gp.num_examples())
    np.random.seed(1)
    np.random.shuffle(perm)
    x_train = np.array(observable_list)[perm]
    y_train = np.array(proc_type)[perm]
    
    return x_train, y_train, ggf_size, vbf_size
    
    
def main(ip):

    # Store starting time:
    start_time = time.time()

    # Initialize can compile in ROOT:
    root_init(ip)

    # We want to handle Ctrl+C
    sh = ROOT.TSignalHandler(ROOT.kSigInterrupt, False)
    sh.Add()
    sh.Connect("Notified()", "TROOT", ROOT.gROOT, "SetInterrupt()")

    # *****************************************************
    #// Perform trainigs:
    # *****************************************************

    #if ip.mode == 'training':

    print "--> Load training data ..."
    gp=GlobalParameters()
    
    x_train, y_train, ggf_size, vbf_size = load_data(ip, gp, 'train')
    
    make2dplot(["pth","ptj1","ptj2","mjj","dphijj","yj1","yj2","yjj","zstar","Rptjet"],x_train,ggf_size,gp._num_examples)
    
    print "--> Load development data ..."

    x_devel, y_devel, ggf_size, vbf_size = load_data(ip, gp, 'devel')
    
    y_final=train(gp, ip, x_train, y_train, x_devel, y_devel)
    y_rewrite=[]
    proc_ind = tf.InteractiveSession().run(tf.argmax(y_final, 1))
    for i in range(len(proc_ind)):
        if proc_ind[i]==0:
            y_rewrite.append([1.,0.])
        else:
            y_rewrite.append([0.,1.0])

    #makeplot("ptH", pthlist_test, weights_test, ggf_size, y_rewrite)
    #makeplot("mj1j2", mjjlist_test, weights_test, ggf_size, y_rewrite)
    #makeplot("ptj1", ptj1list_test, weights_test, ggf_size, y_rewrite)
    #makeplot("yj1", yj1list_test, weights_test, ggf_size, y_rewrite)
    #makeplot("dphijj", dphijjlist_test, weights_test, ggf_size, y_rewrite)

    print "Run time: %d seconds" % (time.time() - start_time)


    sys.exit()




def train(gp,param, x_train, y_train, x_test, y_test):

  print "in train"

  indim = len(x_train[0])

  tf.set_random_seed(1)
  
  #sess = tf.InteractiveSession(config=tf.ConfigProto(device_count={"CPU":8}))
  sess = tf.InteractiveSession(config=tf.ConfigProto(inter_op_parallelism_threads=8))
  ## Create a multilayer model.

  ## Input placeholders
  with tf.name_scope('input'):
    x  = tf.placeholder(tf.float32, [None, indim], name='x-input')
    y_ = tf.placeholder(tf.float32, [None, 2], name='y-input')

  # We can't initialize these variables to 0 - the network will get stuck.
  def weight_variable(shape):
    """Create a weight variable with appropriate initialization."""
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

  def bias_variable(shape):
    """Create a bias variable with appropriate initialization."""
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

  def variable_summaries(var):
    """Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""
    with tf.name_scope('summaries'):
      mean = tf.reduce_mean(var)
      tf.summary.scalar('mean', mean)
      with tf.name_scope('stddev'):
        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
      tf.summary.scalar('stddev', stddev)
      tf.summary.scalar('max', tf.reduce_max(var))
      tf.summary.scalar('min', tf.reduce_min(var))
      tf.summary.histogram('histogram', var)

  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):
    """Reusable code for making a simple neural net layer.
    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.
    It also sets up name scoping so that the resultant graph is easy to read,
    and adds a number of summary ops.
    """
    # Adding a name scope ensures logical grouping of the layers in the graph.
    with tf.name_scope(layer_name):
      # This Variable will hold the state of the weights for the layer
      with tf.name_scope('weights'):
        weights = weight_variable([input_dim, output_dim])
        variable_summaries(weights)
      with tf.name_scope('biases'):
        biases = bias_variable([output_dim])
        variable_summaries(biases)
      with tf.name_scope('Wx_plus_b'):
        preactivate = tf.matmul(input_tensor, weights) + biases
        tf.summary.histogram('pre_activations', preactivate)
      activations = act(preactivate, name='activation')
      tf.summary.histogram('activations', activations)
      return activations

  hidden1 = nn_layer(x, indim, 200, 'layer1')

  hidden2 = nn_layer(hidden1, 200, 50, 'layer2')

  with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    tf.summary.scalar('dropout_keep_probability', keep_prob)
    dropped = tf.nn.dropout(hidden2, keep_prob)

  # Do not apply softmax activation yet, see below.
  y = nn_layer(dropped, 50, 2, 'layer3', act=tf.identity)

  with tf.name_scope('cross_entropy'):
    # The raw formulation of cross-entropy,
    #
    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),
    #                               reduction_indices=[1]))
    #
    # can be numerically unstable.
    #
    # So here we use tf.nn.softmax_cross_entropy_with_logits on the
    # raw outputs of the nn_layer above, and then average across
    # the batch.
    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)
    with tf.name_scope('total'):
      cross_entropy = tf.reduce_mean(diff)
  tf.summary.scalar('cross_entropy', cross_entropy)

  with tf.name_scope('train'):
    train_step = tf.train.AdamOptimizer(param.learning_rate).minimize(
        cross_entropy)
#    train_step = tf.train.AdagradOptimizer(param.learning_rate).minimize(
#        cross_entropy)
    #train_step = tf.train.GradientDescentOptimizer(0.5).minimize(
    #     cross_entropy)

  with tf.name_scope('accuracy'):
    with tf.name_scope('correct_prediction'):
      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    with tf.name_scope('accuracy'):
      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  tf.summary.scalar('accuracy', accuracy)

  ## Merge all the summaries and write them out to
  ## /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)
  merged = tf.summary.merge_all()
  train_writer = tf.summary.FileWriter(param.log_dir + '/train/'+param.run_dir, sess.graph)
  test_writer  = tf.summary.FileWriter(param.log_dir + '/test/'+param.run_dir)
  tf.global_variables_initializer().run()

  # Train the model, and also write summaries.
  # Every 10th step, measure test-set accuracy, and write test summaries
  # All other steps, run train_step on training data, & add training summaries

  def next_batch(gp, batch_size, xs, ys):
    start = gp.index_in_epoch()

    if start + batch_size > gp.num_examples():
        #Finished epoch
        gp._epochs_completed +=1
        # Get the rest examples in this epoch
        rest_num_examples = gp.num_examples() - start
        xs_rest_part = xs[start:gp.num_examples()]
        ys_rest_part = ys[start:gp.num_examples()]

        start = 0
        gp._index_in_epoch = batch_size - rest_num_examples
        end = gp.index_in_epoch()
        xs_new_part = xs[start:end]
        ys_new_part = ys[start:end]
        xs=np.concatenate((xs_rest_part, xs_new_part), axis=0)
        ys=np.concatenate((ys_rest_part, ys_new_part), axis=0)
        return xs, ys
    else:
        gp._index_in_epoch += batch_size
        end = gp.index_in_epoch()
    return xs[start:end], ys[start:end]

  def feed_dict(gp,train):
    """Make a TensorFlow feed_dict: maps data onto Tensor placeholders."""
    if train:
      xs, ys = next_batch(gp, 100, x_train, y_train)
      k = param.dropout
    else:
      xs, ys = x_test, y_test
      k = 1.0
    return {x: xs, y_: ys, keep_prob: k}


  for i in range(param.max_steps):
    if i % 10 == 0:  # Record summaries and test-set accuracy
      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(gp,False))
      test_writer.add_summary(summary, i)
      print('Accuracy at step %s: %s' % (i, acc))
      
    else:  # Record train set summaries, and train
      if i % 100 == 99:  # Record execution stats
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        summary, _ = sess.run([merged, train_step],
                              feed_dict=feed_dict(gp,True),
                              options=run_options,
                              run_metadata=run_metadata)
        train_writer.add_run_metadata(run_metadata, 'step%03d' % i)
        train_writer.add_summary(summary, i)
        print('Adding run metadata for', i)
      else:  # Record a summary
        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(gp,True))
        train_writer.add_summary(summary, i)
    if i==param.max_steps-1:
        summary, acc, y, y_ = sess.run([merged, accuracy, y, y_], feed_dict=feed_dict(gp, False))
        ysoft= tf.nn.softmax(y)
        
        print "Total number of events " , len(x_test)
        nr_ggf=0
        nr_vbf=0
        nr_ggf_rec=0
        nr_vbf_rec=0
        for i in range(len(x_test)):
            if y_[i][0]==0.0:
                nr_vbf +=1
            elif y_[i][0]==1.0:
                nr_ggf+=1
            if y[i][0]>y[i][1]:
                nr_ggf_rec+=1
            elif y[i][0]<y[i][1]:
                nr_vbf_rec+=1
                
        print "Number of GGF events ", nr_ggf
        print "Number of VBF events ", nr_vbf
        print "Number of reconstructed GGF events ", nr_ggf_rec
        print "Number of reconstructed VBF events ", nr_vbf_rec
        print ""        

        prob_range = [0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99]
        
        for prob in prob_range:
          x_new=[]
          y_new=[]
          yuscore_new=[]
          for i in range(len(x_test)):
              if sess.run(ysoft)[i][0] >prob or sess.run(ysoft)[i][1] >prob:
                x_new.append(x_test[i])
                y_new.append(y[i])
                yuscore_new.append(y_[i])

          nr_ggf_new=0
          nr_vbf_new=0
          nr_ggf_rec_new=0
          nr_vbf_rec_new=0
          for i in range(len(x_new)):
              if yuscore_new[i][0]==0.0:
                  nr_vbf_new +=1
              elif yuscore_new[i][0]==1.0:
                  nr_ggf_new+=1
              if y_new[i][0]>y_new[i][1]:
                  nr_ggf_rec_new+=1
              elif y_new[i][0]<y_new[i][1]:
                  nr_vbf_rec_new+=1        

          print ""
          print " Results after cut on probability:", prob        
          print "Total number of events ", len(x_new)
          print "Number of GGF events ", nr_ggf_new
          print "Number of VBF events ", nr_vbf_new
          print "Number of reconstructed GGF events ", nr_ggf_rec_new
          print "Number of reconstructed VBF events ", nr_vbf_rec_new        
        
          y_new=np.array(y_new)
          yuscore_new=np.array(yuscore_new)
          correct_prediction = tf.equal(tf.argmax(y_new, 1), tf.argmax(yuscore_new, 1))
          accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
          print "accuracy_new", sess.run(accuracy)
        sys.exit()
  train_writer.close()
  test_writer.close()

  return sess.run(ysoft)



class InputParameters:

    def __init__(self):

        ## Argument parser

        parser = argparse.ArgumentParser(description='NLO NTuples machine learning tool.') #,formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        #subparser = parser.add_subparsers(dest='MODE', help='Program running mode: training, evaluation')

        #parser_train = subparser.add_parser('training', help='training help (add this mode for more specific help)')
        parser.add_argument("-m", "--multip", dest="MULTIP", required=True, help="Multiplicity of process to reweight: 2, 3")
        parser.add_argument("-e", "--events", dest="EVENTS", default=-1, help="Number of events to be processed [all]")
        parser.add_argument("--debug",  dest="DEBUG", default=False, action='store_const', const=True, help="Generate debug output [False]")
        parser.add_argument("-ggf_train", "--ggf_train", dest="GGFFILE_TRAIN", required=True, help="Root Ntuple for training GGF")
        parser.add_argument("-vbf_train", "--vbf_train", dest="VBFFILE_TRAIN", required=True, help="Root Ntuple for training VBF")
        parser.add_argument("-ggf_eval", "--ggf_eval", dest="GGFFILE_EVAL", required=True, help="Root Ntuple for evaluating GGF")
        parser.add_argument("-vbf_eval", "--vbf_eval", dest="VBFFILE_EVAL", required=True, help="Root Ntuple for evaluating VBF")
        parser.add_argument('--learning_rate', type=float, dest="learning_rate", default=0.0001,help='Initial learning rate')
        parser.add_argument('--log_dir', type=str, dest='log_dir', default='/data/greiner/NTupleAnalyzer/tensorflow',help='Summaries log directory')
        parser.add_argument('--run_dir', type=str, dest='run_dir', default='run1',help='Log directory for single run')
        parser.add_argument('--dropout', type=float, dest="dropout", default=0.9,help='Keep probability for training dropout.')
        parser.add_argument('--max_steps', type=int, dest="max_steps", default=2000,
                      help='Number of steps to run trainer.')

        #parser_ana = subparser.add_parser('analysis', help='analysis help (add this mode for more specific help)')
        #parser_ana.add_argument("-m", "--multip", dest="MULTIP", required=True, help="Multiplicity of process to reweight: 1, 2, 3")
        #parser_ana.add_argument("-s", "--suffix", dest="SUFFIX", default="_new", help="Suffix for output file name [_new]")
        #parser_ana.add_argument("-o", "--output", dest="OUTPUT", default="plots.root", help="name of output file [plots.root]")
        #parser_ana.add_argument("-f", "--folder", dest="FOLDER", default="", help="Output folder if different from input ones")
        #parser_ana.add_argument("-e", "--events", dest="EVENTS", default=-1, help="Number of events to be processed [all]")
        #parser_ana.add_argument("--debug",  dest="DEBUG", default=False, action='store_const', const=True, help="Generate debug output [False]")
        #parser_ana.add_argument("--sourcepath", dest="SOURCEPATH", default="", help="Path to the source of NtupleAnalyzer code")

        #parser_rwgt = subparser.add_parser('reweight', help='reweight help (add this mode for more specific help)')
        #parser_rwgt.add_argument("-m", "--multip", dest="MULTIP", required=True, help="Multiplicity of process to reweight: 1, 2, 3")
        #parser_rwgt.add_argument("-s", "--suffix", dest="SUFFIX", default="_new", help="Suffix for output file name [_new]")
        #parser_rwgt.add_argument("-o", "--output", dest="OUTPUT", default="", help="Output file name. By default the input file name is used with suffix '_new'")
        #parser_rwgt.add_argument("-f", "--folder", dest="FOLDER", default="", help="Output folder if different from input ones")
        #parser_rwgt.add_argument("-e", "--events", dest="EVENTS", default=-1, help="Number of events to be processed [all]")
        #parser_rwgt.add_argument("--debug",  dest="DEBUG", default=False, action='store_const', const=True, help="Generate debug output [False]")
        #parser_rwgt.add_argument("--sourcepath", dest="SOURCEPATH", default="", help="Path to the source of NtupleAnalyzer code")
        #parser_rwgt.add_argument("--noolp",  dest="NOOLP", default=False, action='store_const', const=True, help="Run without link to OLP library [False]")

        #parser.add_argument('INPUTFILES', nargs='+', metavar='Ntuple.root', help='One or more Root NTuple input files. When more than one input file is given, the files are processed one after the other.')
        #parser.add_argument("-s","--show", action='store_true', dest="SHOW", default=False, help="show results in default web browser [NO]")
        #parser.add_argument("-w","--overwrite", action='store_true', dest="OVERWRITE", default=False, help="overwrite existing plots [NO]")
        #parser.add_argument("-n", "-j", "--num-threads",action="store", dest='NUM_THREADS', type=int, default=numcores, help="max number of threads to be used [%s]" % numcores)
        #parser.add_argument("-a", "--alpha", action="store", dest='ALPHA_BAND', type=float, default=0.2, help="transparency of error bands [0.2]")
        #parser.add_argument("-v", "--verbose", action="store_const", const=logging.DEBUG, dest="LOGLEVEL", default=logging.INFO, help="print debug (very verbose) messages [NO]")
        #parser.add_argument("-q", "--quiet", action="store_const", const=logging.WARNING, dest="LOGLEVEL", default=logging.INFO, help="be very quiet [NO]")
        #parser.add_argument("-p", "--prefix", dest="PREFIX", default="xxx", help="prefix to histogram file names")

        args            = parser.parse_args()
        #self.mode       = args.MODE
        #self.filenames  = args.INPUTFILES
        self.multip     = int(args.MULTIP)
        #self.suffix     = args.SUFFIX
        #self.outfile    = args.OUTPUT
        #self.outfolder  = args.FOLDER
        self.events     = args.EVENTS
        self.debug      = args.DEBUG
        #self.sourcepath = args.SOURCEPATH
        self.GGFFILE_TRAIN    = args.GGFFILE_TRAIN
        self.VBFFILE_TRAIN    = args.VBFFILE_TRAIN
        self.GGFFILE_EVAL    = args.GGFFILE_EVAL
        self.VBFFILE_EVAL    = args.VBFFILE_EVAL
        self.learning_rate   = args.learning_rate
        self.log_dir         = args.log_dir
        self.run_dir         = args.run_dir
        self.dropout         = args.dropout
        self.max_steps  = args.max_steps

        #if self.outfolder:
            #if not os.path.isdir(self.outfolder):
                #print "Output folder does not exist, creating it.."
                #os.makedirs(self.outfolder)

        try:
            value = int(self.multip)
        except ValueError:
            print "Multiplicity must be an integer: 2 or 3"
            sys.exit(2)

        #self.filenames.sort()


    def print_parameters(self):

        print "------------------------------"
        print "--    SETUP PARAMETERS      --"
        print "------------------------------"
        print ""
        print (" MODE: Higgs + {0} jet(s)".format( str(self.multip)))
        print ""
        #print " INPUT FILES:"
        #for i in self.filenames:
            #print " -", i
        #print ""
        #print " OUTPUT:"
        #if not self.outfolder:
            #print "   folder: not specified, use same as input-file folder"
        #else:
            #print "   folder: ", self.outfolder
        #if not self.outfile:
            #print "   suffix: ", self.suffix
        #else:
            #print "   file name: ", self.outfile
        #print ""
        #if self.events < 0:
            #print " EVENTS: all"
        #else:
            #print " EVENTS: ", self.events
        #print ""


if __name__ == '__main__':

    input_parameters = InputParameters()
    input_parameters.print_parameters()

    main(input_parameters)


